version: 2.1


executors:
  py36:
    docker:
      - image: prashantsail/pytorch-serve-build
    environment:
      _JAVA_OPTIONS: "-Xmx2048m"
  conda37:
    docker:
      - image: prashantsail/pytorch-serve-build:conda37
    environment:
      _JAVA_OPTIONS: "-Xmx2048m"
  pyenv37:
    docker:
      - image: prashantsail/pytorch-serve-build:pyenv37
    environment:
      _JAVA_OPTIONS: "-Xmx2048m"
  venv36:
    docker:
      - image: prashantsail/pytorch-serve-build:venv36
    environment:
      _JAVA_OPTIONS: "-Xmx2048m"

commands:
  attach-torchserve-workspace:
    description: "Attach the torchserve directory which was saved into workspace"
    steps:
      - attach_workspace:
          at: .

  install-torchserve:
    description: "Install torchserve from a wheel"
    steps:
      - run:
          name: Install torchserve
          command: pip install dist/*.whl

  install-torch-model-archiver:
    description: "Install torch-model-archiver from a wheel"
    steps:
      - run:
          name: Install torch-model-archiver
          command: |
            cd model-archiver
            python setup.py bdist_wheel --universal
            pip install dist/*.whl

  exeucute-api-tests:
    description: "Execute API tests from a collection"
    parameters:
      collection:
        type: enum
        enum: [management, inference, https]
        default: management
    steps:
      - run:
          name: Start torchserve, Execute << parameters.collection >> API Tests, Stop torchserve
          command: .circleci/scripts/linux_test_api.sh << parameters.collection >>
          when: always


jobs:
    build:
      parameters:
        executor:
          type: executor
      executor: << parameters.executor >>
      steps:
        - checkout
        - run:
            name: Build Pytorch Serve
            command: .circleci/scripts/linux_build.sh
        - store_artifacts:
            name: Store gradle testng results
            path: frontend/server/build/reports/tests/test
        - persist_to_workspace:
            root: .
            paths:
              - .

    python-tests:
      parameters:
        executor:
          type: executor
      executor: << parameters.executor >>
      steps:
        - attach-torchserve-workspace
        - run:
            name: Execute python lint and unit tests
            command: .circleci/scripts/linux_test_python.sh
        - store_artifacts:
            name: Store python Test results
            path: result_units
            destination: units

    sanity-tests:
      parameters:
        executor:
          type: executor
      executor: << parameters.executor >>
      steps:
        - attach-torchserve-workspace
        - install-torchserve
        - run:
            name: Execute sanity tests
            command: .circleci/scripts/linux_test_sanity.sh
        - store_artifacts:
            name: Store TS logs from sanity tests
            path: logs/

    api-tests:
      parameters:
        executor:
          type: executor
      executor: << parameters.executor >>
      steps:
        - attach-torchserve-workspace
        - install-torchserve
        - exeucute-api-tests:
            collection: management
        - exeucute-api-tests:
            collection: inference
        - exeucute-api-tests:
            collection: https
        - store_artifacts:
            name: Store server logs and test results
            path: test/artifacts/

    regression-tests:
      parameters:
        executor:
          type: executor
      executor: << parameters.executor >>
      steps:
        - attach-torchserve-workspace
        - install-torchserve
        - install-torch-model-archiver
        - run:
            name: Execute regression suite
            command: .circleci/scripts/linux_test_regression.sh
        - store_artifacts:
            name: Store server logs from regression tests
            path: test/pytest/logs/

    benchmark:
      parameters:
        executor:
          type: executor
      executor: << parameters.executor >>
      steps:
        - attach-torchserve-workspace
        - install-torchserve
        - run:
            name: Start torchserve, Execute benchmark tests, Stop torchserve
            command: .circleci/scripts/linux_test_benchmark.sh
        - store_artifacts:
            name: Store server logs from benchmark tests
            path: logs/
        - store_artifacts:
            name: Store Benchmark Latency resnet-18 results
            path: /tmp/TSBenchmark

    performance-regression:
      parameters:
        executor:
          type: executor
      executor: << parameters.executor >>
      steps:
        - attach-torchserve-workspace
        - install-torchserve
        - run:
            name: Start TS, Execute performance regression testcases, Stop TS
            command: .circleci/scripts/linux_test_perf_regression.sh
        - store_artifacts:
            name: Store server logs
            path: test/performance/logs/
        - store_artifacts:
            name: Store artifacts from performance regression run
            path: test/performance/run_artifacts/
        - store_test_results:
            name: Store performance regression test results for CircleCI
            path: test/performance/run_artifacts/report/

    modelarchiver:
      parameters:
        executor:
          type: executor
      executor: << parameters.executor >>
      steps:
        - checkout
        - run:
            name: Execute python lint, unit and integration tests
            command: .circleci/scripts/linux_test_modelarchiver.sh
        - store_artifacts:
            name: Store unit tests results from model archiver tests
            path: model-archiver/result_units
            destination: units


workflows:
  version: 2

  smoke:
    jobs:
      - &build
        build:
          name: build-<< matrix.executor >>
          matrix: &matrix
            parameters:
              executor: ["py36", "conda37", "pyenv37", "venv36"]
      - &modelarchiver
        modelarchiver:
          name: modelarchiver-<< matrix.executor >>
          matrix: *matrix
      - &python-tests
        python-tests:
          name: python-tests-<< matrix.executor >>
          requires:
            - build-<< matrix.executor >>
          matrix: *matrix
      - &sanity-tests
        sanity-tests:
          name: sanity-tests-<< matrix.executor >>
          requires:
            - build-<< matrix.executor >>
          matrix: *matrix
      # added to test nightly on different env todo remove
      - &api-tests
        api-tests:
          name: api-tests-<< matrix.executor >>
          requires:
            - build-<< matrix.executor >>
          matrix: *matrix
      - &regression-tests
        regression-tests:
          name: regression-tests-<< matrix.executor >>
          requires:
            - build-<< matrix.executor >>
          matrix: *matrix

  nightly:
    triggers:
      - schedule:
          cron: "0 0 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - *build
      - *modelarchiver
      - *python-tests
      - *sanity-tests
      - &api-tests
        api-tests:
          name: api-tests-<< matrix.executor >>
          requires:
            - build-<< matrix.executor >>
          matrix: *matrix
      - &regression-tests
        regression-tests:
          name: regression-tests-<< matrix.executor >>
          requires:
            - build-<< matrix.executor >>
          matrix: *matrix

  weekly:
    triggers:
      - schedule:
          cron: "0 0 * * 0"
          filters:
            branches:
              only:
                - master
    jobs:
      - *build
      - benchmark:
          name: benchmark-<< matrix.executor >>
          requires:
            - build-<< matrix.executor >>
          matrix: *matrix
#      Uncomment once performance regression PR gets merged
#      - performance-regression:
#          name: performance-regression-<< matrix.executor >>
#          requires:
#            - build-<< matrix.executor >>
#          matrix: *matrix
